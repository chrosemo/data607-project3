---
title: "linkedin"
author: "Mael Illien"
date: "10/14/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(rvest)
library(tidyverse)
```

```{r}
wlist <- read.csv('https://raw.githubusercontent.com/dhairavc/data607-project3/master/whitelist.csv')
```


```{r}
extract_words <- function(link) {
  # download the html and turn it into an XML file with read_html()
  job <- read_html(link)
  # extract specific nodes with html_nodes() using css selector
  skills <- html_nodes(job, ".description__text--rich li")
  # extract content from nodes
  skills <- html_text(skills)
  # remove punctuation
  words <- gsub('[[:punct:]]', '', skills)
  # split sentences into individual words
  words <- unlist(strsplit(words, " "))
  words <- tolower(words)
  #words <- words[words %in% wlist$Whitelist]
  # count the number of occurences of each word
  wordcount <- table(words)
  wordcount_df <- as.data.frame(wordcount)
  return(wordcount_df)
}
```

```{r}
# save the url which contains the search results
rooturl <- "https://www.linkedin.com/jobs/data-scientist-jobs/"
# for each job, extract the href attribute from each job using the css selector
jobs <- read_html(rooturl)
links <- html_nodes(jobs, "a.result-card__full-card-link")
links <- html_attr(links, "href")


#loop through pages and append to links vector
for (x in 1:39) {
  paged_url <- "https://www.linkedin.com/jobs/search/?keywords=data%20scientist&start="
  paged_url <- paste(paged_url, 25*x, collapse = NULL, sep = "")
  
  jobs <- read_html(paged_url)
  links_temp <- html_nodes(jobs, "a.result-card__full-card-link")
  links_temp <- html_attr(links_temp, "href")
  links <- c(links, links_temp)

}




```


```{r}
str(links)
links[1000]
```
```{r}
# apply the extract_words function to each link
counts <- list()
for (i in 1:length(links)) {
    df <- extract_words(links[i])
    counts[[i]] <- df
}
# combine into a dataframe
skill_count <- do.call(rbind, counts)
```

```{r}
# sum multiple occurences of the same word
total_skill_count <- skill_count %>% 
  group_by(words) %>% 
  summarize(Occurences = sum(Freq)) %>% 
  arrange(desc(Occurences ))
total_skill_count
```
```{r}
# apply whitelist
total_skill_count %>% filter(words %in% wlist$Whitelist)
```